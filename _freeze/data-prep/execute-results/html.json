{
  "hash": "3f6b21e54941ee110038d7d5913dc411",
  "result": {
    "markdown": "---\nexecute: \n  eval: false\n---\n\n# Preparation & Exploration\n\n## Vorgehen\n\nBevor ich mit Altair Visualisierungen und meine RevealJS oder Streamlit-App erstellt habe, hatte ich selbstverständlich zuvor den Datensatz mithilfe von Pandas in Jupyter Notebooks geladen und mit unterschiedlichen Hilfsfunktionen den Dataframe und die Spalten analysiert:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\ndf.info()\n\nfor col in df.columns.tolist():\n    print('')\n    print('')\n    print(f'Column: {col}')\n    print('--- describe ---------------')\n    print(df[col].describe())\n    print('--- unique -----------------')\n    print(df[col].unique().tolist())\n    print('--- value counts -----------')\n    print(df[col].value_counts())\n```\n:::\n\n\n::: {style=\"padding-top: 20px;\"}\n:::\n\nRausgekommen ist dabei, dass der Datensatz aus 18 Spalten mal 3900 Zeilen besteht. Eine Zeile steht dabei für einen Einkauf, die jeweils von einem eindeutigen Kunden getätigt wurde. Da eine dieser Zeilen die ID eines Kunden darstellen soll, hat diese keinen Wert für die Analyse und wird somit aus dem Dataframe entfernt. Zeitgleich sollen auch alle „null-values“ entfernt werden, die bei diesem Datensatz jedoch nicht existieren, aber aufgrund von Vollständigkeit und Fehlervermeidung versucht entfernt zu werden:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# remove ID column\ndf = df.drop(columns=['Customer ID']) if 'Customer ID' in df.columns else df\n\n# remove rows with missing values\ndf = df.dropna() if df.isnull().values.any() else df\n```\n:::\n\n\n::: {style=\"padding-top: 20px;\"}\n:::\n\n\nUm Altersunterschiede zu untersuchen habe ich dem Dataframe die Spalte „Age Group“ hinzugefügt, dessen Größe der Bins ich in der Alterspanne von ähnlichen Lebensumständen gewählt habe.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Define age ranges\nbins = [0, 26, 36, 46, 56, 66, 71]\nlabels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66-70']\ndf['Age Group'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)\n```\n:::\n\n\n::: {style=\"padding-top: 20px;\"}\n:::\n\n\nDa Daten zum Standort der Kunden existieren, ist eine Map Chart der ideale Weg, geographische Zusammenhänge zu untersuchen und allgemein die Standorte zu veranschaulichen. Hierfür haben aber die Latitude und Longitude gefehlt, um diese mit Altair zu visualisieren. Aufgrund von Fehler und dem sehr hohen Aufwand, diese Informationen durch eine Python Library in Realtime für die Visualisierung im Python Skript zu laden, habe ich dies eigenhändig in das Dataframe eingefügt. Die Latitude und Longitude habe ich dann mithilfe einer API recht schnell für jeden Bundesstaat herausfinden und nachtragen können und damit die zwei fehlenden Spalten zum Dataframe hinzugefügt (verkürzt, schemafolgend für alle Locations):\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# add longitude and latitude\ncoordinates_data = [\n    {\"Location\": \"Montana\", \"Latitude\": 46.879681, \"Longitude\": -110.362564},\n    {\"Location\": \"California\", \"Latitude\": 36.778259, \"Longitude\": -119.417931},\n    {\"Location\": \"Idaho\", \"Latitude\": 44.068203, \"Longitude\": -114.742043},\n    {\"Location\": \"Illinois\", \"Latitude\": 40.633125, \"Longitude\": -89.398529},\n    # ...\n]\n\n# Erstelle ein neues DataFrame für die manuell eingegebenen Koordinaten\ncoordinates_df = pd.DataFrame(coordinates_data)\n\n# Füge die manuell eingegebenen Koordinaten zum ursprünglichen DataFrame hinzu\ndf = pd.merge(df, coordinates_df, on=\"Location\", how=\"left\")\n```\n:::\n\n\n::: {style=\"padding-top: 20px;\"}\n:::\n\n\nGeplant hatte ich auch, mithilfe eines Line Charts eine zeitliche Entwicklung innerhalb eines Jahres der Einkäufe aufzuzeigen. Diese wurde jedoch am Ende aufgrund der unklaren und knappen Informationen über den Zeitpunkt der Einkäufe, die ich nur über die Jahreszeit erhalten konnte, doch nicht aufgezeigt. Hierzu konnte ich auch nichts Relevantes im Datensatz finden. Um ein konkretes Datum für die Einkäufe zu erhalten habe ich je nach Jahreszeit das Anfangsdatum der Jahreszeit als „Einkaufsdatum“ benutzt (zumindest nur anfangs geplant):\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Funktion zur Berechnung des passenden Datums für jede Jahreszeit\ndef get_season_date(season):\n    if season == 'Spring':\n        return pd.to_datetime('2023-03-21')\n    elif season == 'Summer':\n        return pd.to_datetime('2023-06-21')\n    elif season == 'Fall':\n        return pd.to_datetime('2023-09-23')\n    elif season == 'Winter':\n        return pd.to_datetime('2023-12-21')\n    else:\n        return None\n\n# Füge eine neue Spalte 'Season Date' hinzu\ndf['Season Date'] = df['Season'].apply(get_season_date)\n\ndf['Season Date'] = pd.to_datetime(df['Season Date'], format='%Y-%m-%d')\n```\n:::\n\n\n::: {style=\"padding-top: 20px;\"}\n:::\n\n\nSchließlich habe ich noch für die Nutzung der Spalten mit Altair alle ordinalen und nominalen Spalten als Typ „category“ deklariert:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# convert categorical columns\ncategorical_columns = ['Gender', 'Item Purchased', 'Category', 'Location', 'Size', 'Color', 'Season', 'Subscription Status', 'Shipping Type', 'Discount Applied', 'Promo Code Used', 'Payment Method', 'Frequency of Purchases', 'Age Group']\ndf[categorical_columns] = df[categorical_columns].astype('category')\n```\n:::\n\n\n::: {style=\"padding-top: 20px;\"}\n:::\n\n\n## Ergebnisse der Exploration/Preparation\n\nMit den Homeworks hatte ich nun einige Erfahrungen mit der Visualisierung durch Altair und der Aufbau einer Präsentation mit Quarto, explizit RevealJS, erhalten können, wodurch ich einen anderen Blickwinkel auf die Daten nach den Homeworks hatte als vor den Homeworks. Beim gründlicheren Analysieren des Datensatzes wurde mir bewusst, dass der Datensatz doch einige Herausforderungen im Blick auf lehrreiche Rückschlüsse mit sich bringt, da die Daten über die jeweiligen Einkäufe sehr limitiert waren. Dies schien für mich nicht sonderlich repräsentativ für die Daten in einem echten Shopping Unternehmen:\n\n1.\tEs gibt eine Spalte „Previous Purchases“, die besagt, wie viele Einkäufe der Kunde zuvor getätigt hat. Informationen über diese bisherigen Einkäufe gibt es leider nicht. Dies hätte sicher interessante Einblicke auf das Kaufverhalten von Kunden über einen Zeitverlauf gegeben.\n\n2.\tEs wurde immer nur ein Gegenstand erworben, was in der Realität beinahe unmöglich ist, dass bei keinem Einkauf mehr als ein Gegenstand erworben wurde, da diverse Verkaufsgegenstände in „Spar-Bundles“ (o.ä.) verkauft werden, viele Gegenstände nur in Kombination mit anderen Gegenständen gut aussehen / funktionieren / ein Ganzes ergeben und man sich dutzende Einzellieferungen spart. Hierbei wäre es interessant herauszufinden, welche Kombinationen an Kleidungsstücken im Trend sind.\n\n3.\tDas konkrete Datum eines Einkaufs war auch nicht hinterlegt, nur die Jahreszeit in dem der Einkauf stattfand. Hier hätte man sicherlich interessante Informationen in Bezug auf beliebte Einkaufstage erhalten können. \n\n4.\tAllgemein waren die Daten sehr gleichmäßig verteilt und man hat kaum Trends zueinander gefunden. Nur bei wenigen Spalten konnte man bei den Ausprägungen deutlich erkennen, dass diese einen höheren Umsatz als die anderen Ausprägungen untereinander erzeugten.\n\n",
    "supporting": [
      "data-prep_files"
    ],
    "filters": [],
    "includes": {}
  }
}