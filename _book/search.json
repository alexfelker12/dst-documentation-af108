[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projekt Dokumentation",
    "section": "",
    "text": "Einführung - Gedanken - Motivation\nAuf der Suche nach einem Datensatz für das Projekt bin ich auf die Website Kaggle gestoßen, bei der viele, schon zu einem gewissen Grad aufbereitete, Datensätze für Machine-Learning Zwecke zur freien privaten Nutzung zur Verfügung stehen. Für die Bearbeitung des Projekts wurden ca. 20 Attribute mit unterschiedlich quantitativen und qualitativen Spalten empfohlen. Daraufhin hielte ich nach Datensätzen, die diesen Anforderungen entsprechen, Ausschau und hatte neben dem Invistico Airline Customer Satisfaction Datensatz, welches eines der im „Big Idea Worksheet“ ausgewählte Datensatz als Kandidat für das Projekt zur Verfügung stand, den Customer Shopping Trends Datensatz gefunden, welches der andere Kandidat für das Projekt darstellte.\nEntschieden habe ich mich für den „Customer Shopping Trends“ Datensatz, da ich in diesem Datensatz mehr Potenzial für aussagekräftigere Visualisierungen und lehrreicher Erkenntnissen sah als beim anderen Kandidaten. Weitere voreilige Schlüsse wollte ich über den ausgewählten Datensatz jedoch nicht ziehen, da ich im Bereich des Datastory-Telling keine Vorkenntnisse besitze, vor allem aus dem Grund, dass ich in meinem Studium mehr auf die technischen und wirtschaftlichen Aspekte in Bezug auf Daten achten musste. Das verständliche Darstellen von Daten, als auch das erzählerische Überbringen von Informationen war und ist zu einem gewissen Teil immer noch eine große Herausforderung für mich.\nDisclaimer: Die in der Dokumentation folgenden Code-Ausschnitte sind mithilfe von diversen Foren, wie Stackoverflow, Stackexchangge etc. und ChatGPT von OpenAI zusammengestellt worden!"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "2  Test page",
    "section": "",
    "text": "Main title"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "test.html#sub-title",
    "href": "test.html#sub-title",
    "title": "Main title",
    "section": "Sub title",
    "text": "Sub title\nthis is placeholder text, fett kursiv Homepage\n\n\n1 + 1"
  },
  {
    "objectID": "learnings.html",
    "href": "learnings.html",
    "title": "3  Fazit & Gelerntes",
    "section": "",
    "text": "Eine besondere Herausforderung war eine richtige Story aus den Erkenntnissen der Analyse und den erstellten Graphen zu bilden. Wie schon anfangs erwähnt gibt es keine nützlichen zeitlichen Informationen weshalb es unmöglich für mich war die Entwicklung der Daten oder zeitlich abhängige Trends Stück für Stück dem Publikum zu veranschaulichen.\nEines der Dinge, die ich besser hätte machen können, war die Reihenfolge der präsentierten Informationen. In diesem Datensatz hatte ich von Länderdaten bis hin zu dem eigentlichen Produkt mehrere Ebenen, die ich mit dem Fortschritt der Präsentation immer abstufen könnte, also beispielsweise: Location -&gt; Category -&gt; Color -&gt; Item Purchased -&gt; … oder so ähnlich."
  },
  {
    "objectID": "data-prep.html",
    "href": "data-prep.html",
    "title": "Preparation & Exploration",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "building-process.html#sub-title",
    "href": "building-process.html#sub-title",
    "title": "Erstellungsprozess",
    "section": "Sub title",
    "text": "Sub title\nthis is placeholder text, fett kursiv Homepage\n\n\n1 + 1"
  },
  {
    "objectID": "building-process.html#dashboard",
    "href": "building-process.html#dashboard",
    "title": "Erstellungsprozess",
    "section": "Dashboard",
    "text": "Dashboard\nthis is placeholder text, fett kursiv Homepage\n\n\n1 + 1"
  },
  {
    "objectID": "building-process.html#präsentation",
    "href": "building-process.html#präsentation",
    "title": "Erstellungsprozess",
    "section": "Präsentation",
    "text": "Präsentation"
  },
  {
    "objectID": "building-process.html",
    "href": "building-process.html",
    "title": "execute:",
    "section": "",
    "text": "Erstellungsprozess"
  },
  {
    "objectID": "data-prep.html#ergebnisse-der-explorationpreparation",
    "href": "data-prep.html#ergebnisse-der-explorationpreparation",
    "title": "1  Preparation & Exploration",
    "section": "1.2 Ergebnisse der Exploration/Preparation",
    "text": "1.2 Ergebnisse der Exploration/Preparation\nMit den Homeworks hatte ich nun einige Erfahrungen mit der Visualisierung durch Altair und der Aufbau einer Präsentation mit Quarto, explizit RevealJS, erhalten können, wodurch ich einen anderen Blickwinkel auf die Daten nach den Homeworks hatte als vor den Homeworks. Beim gründlicheren Analysieren des Datensatzes wurde mir bewusst, dass der Datensatz doch einige Herausforderungen im Blick auf lehrreiche Rückschlüsse mit sich bringt, da die Daten über die jeweiligen Einkäufe sehr limitiert waren. Dies schien für mich nicht sonderlich repräsentativ für die Daten in einem echten Shopping Unternehmen:\n\nEs gibt eine Spalte „Previous Purchases“, die besagt, wie viele Einkäufe der Kunde zuvor getätigt hat. Informationen über diese bisherigen Einkäufe gibt es leider nicht. Dies hätte sicher interessante Einblicke auf das Kaufverhalten von Kunden über einen Zeitverlauf gegeben.\nEs wurde immer nur ein Gegenstand erworben, was in der Realität beinahe unmöglich ist, dass bei keinem Einkauf mehr als ein Gegenstand erworben wurde, da diverse Verkaufsgegenstände in „Spar-Bundles“ (o.ä.) verkauft werden, viele Gegenstände nur in Kombination mit anderen Gegenständen gut aussehen / funktionieren / ein Ganzes ergeben und man sich dutzende Einzellieferungen spart. Hierbei wäre es interessant herauszufinden, welche Kombinationen an Kleidungsstücken im Trend sind.\nDas konkrete Datum eines Einkaufs war auch nicht hinterlegt, nur die Jahreszeit in dem der Einkauf stattfand. Hier hätte man sicherlich interessante Informationen in Bezug auf beliebte Einkaufstage erhalten können.\nAllgemein waren die Daten sehr gleichmäßig verteilt und man hat kaum Trends zueinander gefunden. Nur bei wenigen Spalten konnte man bei den Ausprägungen deutlich erkennen, dass diese einen höheren Umsatz als die anderen Ausprägungen untereinander erzeugten."
  },
  {
    "objectID": "data-prep.html#vorgehen",
    "href": "data-prep.html#vorgehen",
    "title": "1  Preparation & Exploration",
    "section": "1.1 Vorgehen",
    "text": "1.1 Vorgehen\nBevor ich mit Altair Visualisierungen und meine RevealJS oder Streamlit-App erstellt habe, hatte ich selbstverständlich zuvor den Datensatz mithilfe von Pandas in Jupyter Notebooks geladen und mit unterschiedlichen Hilfsfunktionen den Dataframe und die Spalten analysiert:\n\ndf.info()\n\nfor col in df.columns.tolist():\n    print('')\n    print('')\n    print(f'Column: {col}')\n    print('--- describe ---------------')\n    print(df[col].describe())\n    print('--- unique -----------------')\n    print(df[col].unique().tolist())\n    print('--- value counts -----------')\n    print(df[col].value_counts())\n\n\n\n\nRausgekommen ist dabei, dass der Datensatz aus 18 Spalten mal 3900 Zeilen besteht. Eine Zeile steht dabei für einen Einkauf, die jeweils von einem eindeutigen Kunden getätigt wurde. Da eine dieser Zeilen die ID eines Kunden darstellen soll, hat diese keinen Wert für die Analyse und wird somit aus dem Dataframe entfernt. Zeitgleich sollen auch alle „null-values“ entfernt werden, die bei diesem Datensatz jedoch nicht existieren, aber aufgrund von Vollständigkeit und Fehlervermeidung versucht entfernt zu werden:\n\n# remove ID column\ndf = df.drop(columns=['Customer ID']) if 'Customer ID' in df.columns else df\n\n# remove rows with missing values\ndf = df.dropna() if df.isnull().values.any() else df\n\n\n\n\nUm Altersunterschiede zu untersuchen habe ich dem Dataframe die Spalte „Age Group“ hinzugefügt, dessen Größe der Bins ich in der Alterspanne von ähnlichen Lebensumständen gewählt habe.\n\n# Define age ranges\nbins = [0, 26, 36, 46, 56, 66, 71]\nlabels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66-70']\ndf['Age Group'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)\n\n\n\n\nDa Daten zum Standort der Kunden existieren, ist eine Map Chart der ideale Weg, geographische Zusammenhänge zu untersuchen und allgemein die Standorte zu veranschaulichen. Hierfür haben aber die Latitude und Longitude gefehlt, um diese mit Altair zu visualisieren. Aufgrund von Fehler und dem sehr hohen Aufwand, diese Informationen durch eine Python Library in Realtime für die Visualisierung im Python Skript zu laden, habe ich dies eigenhändig in das Dataframe eingefügt. Die Latitude und Longitude habe ich dann mithilfe einer API recht schnell für jeden Bundesstaat herausfinden und nachtragen können und damit die zwei fehlenden Spalten zum Dataframe hinzugefügt (verkürzt, schemafolgend für alle Locations):\n\n# add longitude and latitude\ncoordinates_data = [\n    {\"Location\": \"Montana\", \"Latitude\": 46.879681, \"Longitude\": -110.362564},\n    {\"Location\": \"California\", \"Latitude\": 36.778259, \"Longitude\": -119.417931},\n    {\"Location\": \"Idaho\", \"Latitude\": 44.068203, \"Longitude\": -114.742043},\n    {\"Location\": \"Illinois\", \"Latitude\": 40.633125, \"Longitude\": -89.398529},\n    # ...\n]\n\n# Erstelle ein neues DataFrame für die manuell eingegebenen Koordinaten\ncoordinates_df = pd.DataFrame(coordinates_data)\n\n# Füge die manuell eingegebenen Koordinaten zum ursprünglichen DataFrame hinzu\ndf = pd.merge(df, coordinates_df, on=\"Location\", how=\"left\")\n\n\n\n\nGeplant hatte ich auch, mithilfe eines Line Charts eine zeitliche Entwicklung innerhalb eines Jahres der Einkäufe aufzuzeigen. Diese wurde jedoch am Ende aufgrund der unklaren und knappen Informationen über den Zeitpunkt der Einkäufe, die ich nur über die Jahreszeit erhalten konnte, doch nicht aufgezeigt. Hierzu konnte ich auch nichts Relevantes im Datensatz finden. Um ein konkretes Datum für die Einkäufe zu erhalten habe ich je nach Jahreszeit das Anfangsdatum der Jahreszeit als „Einkaufsdatum“ benutzt (zumindest nur anfangs geplant):\n\n# Funktion zur Berechnung des passenden Datums für jede Jahreszeit\ndef get_season_date(season):\n    if season == 'Spring':\n        return pd.to_datetime('2023-03-21')\n    elif season == 'Summer':\n        return pd.to_datetime('2023-06-21')\n    elif season == 'Fall':\n        return pd.to_datetime('2023-09-23')\n    elif season == 'Winter':\n        return pd.to_datetime('2023-12-21')\n    else:\n        return None\n\n# Füge eine neue Spalte 'Season Date' hinzu\ndf['Season Date'] = df['Season'].apply(get_season_date)\n\ndf['Season Date'] = pd.to_datetime(df['Season Date'], format='%Y-%m-%d')\n\n\n\n\nSchließlich habe ich noch für die Nutzung der Spalten mit Altair alle ordinalen und nominalen Spalten als Typ „category“ deklariert:\n\n# convert categorical columns\ncategorical_columns = ['Gender', 'Item Purchased', 'Category', 'Location', 'Size', 'Color', 'Season', 'Subscription Status', 'Shipping Type', 'Discount Applied', 'Promo Code Used', 'Payment Method', 'Frequency of Purchases', 'Age Group']\ndf[categorical_columns] = df[categorical_columns].astype('category')"
  },
  {
    "objectID": "building-process.html#planung-dashboard-präsentation",
    "href": "building-process.html#planung-dashboard-präsentation",
    "title": "Erstellungsprozess",
    "section": "Planung Dashboard & Präsentation",
    "text": "Planung Dashboard & Präsentation\nAus diversen Modulen in meinem Studium hatte ich die Wichtigkeit einer guten Planung für den Erfolg eines Projekts als lehrreichste Erfahrung mitgenommen. Teilweise halte ich die Planung für einige Projekte in meinem Studium jedoch immer noch ein wenig zu klein und kurzsichtig, was sich im Nachhinein immer als Ärgernis darstellt.\nIn diesem Modul hatte ich für die Planung und der schier Unmengen an Möglichkeiten zur Analyse des Datensatzes nach einer Methode Ausschau gehalten, mit denen ich verschiedene Auffälligkeiten im Datensatz schnell entdecken und untersuchen kann. Dabei hatte ich von der Homework mit Streamlit die Idee, Tabellen interaktiv mit der Auswahl von Attributen für die X- und Y-Achse zu erstellen und diese mit verschiedenen Aggregationen und Sortierungen zu kombinieren, wodurch ich zwei Attribute schnell und mit der Leichtigkeit von ein paar Klicks näher untersuchen konnte. Vieles fand aber in einigen Jupyter Notebooks statt, bei dem ich verschiedene Visualisierungen mit Altair getestet habe, um für jede Information, die ich präsentieren will, den passenden Graphtypen zu finden."
  },
  {
    "objectID": "building-process.html#umsetzung-dashboard-präsentation",
    "href": "building-process.html#umsetzung-dashboard-präsentation",
    "title": "Erstellungsprozess",
    "section": "Umsetzung Dashboard & Präsentation",
    "text": "Umsetzung Dashboard & Präsentation\nUm dies umzusetzen habe ich eine Funktion gebraucht, die all diese Parameter nehmen kann und mir einen Altair Chart returned. Mithilfe von ChatGPT und einigen korrigierenden Prompts habe ich diese Funktion schließlich erstellen können:\n\ndef create_chart(df, x='', y='', aggregation='sum', chart_type='bar', title='', sort_order=None, show_text=False):\n    \"\"\"\n    Create a chart based on the specified parameters.\n\n    Parameters:\n        - df: DataFrame, the input DataFrame.\n        - x: str, the column for the x-axis.\n        - y: str, the column for the y-axis.\n        - aggregation: str, the aggregation method ('average', 'sum', 'count', 'min', 'max', 'median', 'stdev', 'variance').\n        - chart_type: str, the type of chart ('bar', 'point', 'line', 'area', 'tick', 'scatter', 'boxplot').\n        - title: str, the title of the chart.\n        - sort_order: str, the sort order for the x-axis ('ascending', 'descending', None).\n        - boxplot: bool, whether to create a boxplot.\n\n    Returns:\n        - alt.Chart: Altair chart object.\n    \"\"\"\n\n    title = f'{aggregation.capitalize()} of {y} by {x}' if title == '' else title\n\n    valid_aggregations = ['', 'average', 'sum', 'count',\n                          'min', 'max', 'median', 'stdev', 'variance']\n    valid_chart_types = ['bar', 'point', 'line',\n                         'area', 'tick', 'scatter', 'boxplot']\n    valid_dtypes = ['int64', 'float64', 'category']\n\n    if aggregation not in valid_aggregations:\n        raise ValueError(\n            f\"Invalid aggregation method. Choose one of {valid_aggregations}.\")\n\n    if chart_type not in valid_chart_types:\n        raise ValueError(\n            f\"Invalid chart type. Choose one of {valid_chart_types}.\")\n\n    if df[x].dtype not in valid_dtypes:\n        raise ValueError(\n            f\"Invalid data type for x. Has to be one of {valid_dtypes}.\")\n\n    chart_type_mapping = {\n        'bar': alt.Chart(df).mark_bar(),\n        'point': alt.Chart(df).mark_point(),\n        'line': alt.Chart(df.sort_values(by=[x])).mark_line(),\n        'area': alt.Chart(df).mark_area(),\n        'tick': alt.Chart(df).mark_tick(),\n        'scatter': alt.Chart(df).mark_circle(),\n        'boxplot': alt.Chart(df).mark_boxplot()\n    }\n\n    x_enc = f'{x}:O' if df[x].dtype == 'category' else f'{x}:Q'\n    boxplot_y_enc = f'{y}:Q'\n    y_enc = f'{aggregation}({y}):Q' if aggregation else f'{y}:O' if df[\n        y].dtype == 'category' else f'{y}:Q'\n\n    if chart_type == 'boxplot':\n        return chart_type_mapping['boxplot'].encode(\n            x=alt.X(x_enc, sort='y' if sort_order ==\n                    'ascending' else '-y' if sort_order == 'descending' else None),\n            y=alt.Y(boxplot_y_enc),\n            color=x_enc,\n            tooltip=[x_enc, y_enc],\n        ).properties(\n            title=title\n        )\n    else:\n        chart = chart_type_mapping[chart_type].encode(\n            x=alt.X(x_enc, sort='y' if sort_order ==\n                    'ascending' else '-y' if sort_order == 'descending' else None),\n            y=alt.Y(y_enc),\n            color=x_enc,\n        )\n\n        text_labels = alt.Chart(df).mark_text(\n            align='center',\n            baseline='bottom',\n            dx=0,\n            fontSize=14,\n        ).encode(\n            x=alt.X(x_enc, sort='y' if sort_order ==\n                    'ascending' else '-y' if sort_order == 'descending' else None),\n            y=alt.Y(y_enc),\n            text=alt.Text(y_enc, format='.2f'),\n            opacity=alt.value(1) if show_text else alt.value(0),\n        )\n\n        return (chart + text_labels).properties(\n            title=title,\n            width='container'\n        )"
  },
  {
    "objectID": "building-process.html#planung",
    "href": "building-process.html#planung",
    "title": "2  Erstellungsprozess",
    "section": "2.1 Planung",
    "text": "2.1 Planung\nAus diversen Modulen in meinem Studium hatte ich die Wichtigkeit einer guten Planung für den Erfolg eines Projekts als lehrreichste Erfahrung mitgenommen. Teilweise halte ich die Planung für einige Projekte in meinem Studium jedoch immer noch ein wenig zu klein und kurzsichtig, was sich im Nachhinein immer als Ärgernis darstellt.\nIn diesem Modul hatte ich für die Planung und der schier Unmengen an Möglichkeiten zur Analyse des Datensatzes nach einer Methode Ausschau gehalten, mit denen ich verschiedene Auffälligkeiten im Datensatz schnell entdecken und untersuchen kann. Dabei hatte ich von der Homework mit Streamlit die Idee, Tabellen interaktiv mit der Auswahl von Attributen für die X- und Y-Achse zu erstellen und diese mit verschiedenen Aggregationen und Sortierungen zu kombinieren, wodurch ich zwei Attribute schnell und mit der Leichtigkeit von ein paar Klicks näher untersuchen konnte. Vieles fand aber in einigen Jupyter Notebooks statt, bei dem ich verschiedene Visualisierungen mit Altair getestet habe, um für jede Information, die ich präsentieren will, den passenden Graphtypen zu finden."
  },
  {
    "objectID": "building-process.html#umsetzung",
    "href": "building-process.html#umsetzung",
    "title": "2  Erstellungsprozess",
    "section": "2.2 Umsetzung",
    "text": "2.2 Umsetzung\nUm dies umzusetzen habe ich eine Funktion gebraucht, die all diese Parameter nehmen kann und mir einen Altair Chart returned. Mithilfe von ChatGPT und einigen korrigierenden Prompts habe ich diese Funktion schließlich erstellen können:\n\n\ndef create_chart(df, x='', y='', aggregation='sum', chart_type='bar', title='', sort_order=None, show_text=False):\n    \"\"\"\n    Create a chart based on the specified parameters.\n\n    Parameters:\n        - df: DataFrame, the input DataFrame.\n        - x: str, the column for the x-axis.\n        - y: str, the column for the y-axis.\n        - aggregation: str, the aggregation method ('average', 'sum', 'count', 'min', 'max', 'median', 'stdev', 'variance').\n        - chart_type: str, the type of chart ('bar', 'point', 'line', 'area', 'tick', 'scatter', 'boxplot').\n        - title: str, the title of the chart.\n        - sort_order: str, the sort order for the x-axis ('ascending', 'descending', None).\n        - boxplot: bool, whether to create a boxplot.\n\n    Returns:\n        - alt.Chart: Altair chart object.\n    \"\"\"\n\n    title = f'{aggregation.capitalize()} of {y} by {x}' if title == '' else title\n\n    valid_aggregations = ['', 'average', 'sum', 'count',\n                          'min', 'max', 'median', 'stdev', 'variance']\n    valid_chart_types = ['bar', 'point', 'line',\n                         'area', 'tick', 'scatter', 'boxplot']\n    valid_dtypes = ['int64', 'float64', 'category']\n\n    if aggregation not in valid_aggregations:\n        raise ValueError(\n            f\"Invalid aggregation method. Choose one of {valid_aggregations}.\")\n\n    if chart_type not in valid_chart_types:\n        raise ValueError(\n            f\"Invalid chart type. Choose one of {valid_chart_types}.\")\n\n    if df[x].dtype not in valid_dtypes:\n        raise ValueError(\n            f\"Invalid data type for x. Has to be one of {valid_dtypes}.\")\n\n    chart_type_mapping = {\n        'bar': alt.Chart(df).mark_bar(),\n        'point': alt.Chart(df).mark_point(),\n        'line': alt.Chart(df.sort_values(by=[x])).mark_line(),\n        'area': alt.Chart(df).mark_area(),\n        'tick': alt.Chart(df).mark_tick(),\n        'scatter': alt.Chart(df).mark_circle(),\n        'boxplot': alt.Chart(df).mark_boxplot()\n    }\n\n    x_enc = f'{x}:O' if df[x].dtype == 'category' else f'{x}:Q'\n    boxplot_y_enc = f'{y}:Q'\n    y_enc = f'{aggregation}({y}):Q' if aggregation else f'{y}:O' if df[\n        y].dtype == 'category' else f'{y}:Q'\n\n    if chart_type == 'boxplot':\n        return chart_type_mapping['boxplot'].encode(\n            x=alt.X(x_enc, sort='y' if sort_order ==\n                    'ascending' else '-y' if sort_order == 'descending' else None),\n            y=alt.Y(boxplot_y_enc),\n            color=x_enc,\n            tooltip=[x_enc, y_enc],\n        ).properties(\n            title=title\n        )\n    else:\n        chart = chart_type_mapping[chart_type].encode(\n            x=alt.X(x_enc, sort='y' if sort_order ==\n                    'ascending' else '-y' if sort_order == 'descending' else None),\n            y=alt.Y(y_enc),\n            color=x_enc,\n        )\n\n        text_labels = alt.Chart(df).mark_text(\n            align='center',\n            baseline='bottom',\n            dx=0,\n            fontSize=14,\n        ).encode(\n            x=alt.X(x_enc, sort='y' if sort_order ==\n                    'ascending' else '-y' if sort_order == 'descending' else None),\n            y=alt.Y(y_enc),\n            text=alt.Text(y_enc, format='.2f'),\n            opacity=alt.value(1) if show_text else alt.value(0),\n        )\n\n        return (chart + text_labels).properties(\n            title=title,\n            width='container'\n        )\n\n\n\nDies funktioniert allgemein so, dass anhand des übergebenen Chart Typs die passende Altair Chart Funktion benutzt wird und aufbauend auf diesem das Encoding und die Properties basierend auf den anderen übergeben Parameter hinzugefügt werden.\nMithilfe dieser Funktion konnte ich eine Vielzahl verschiedener Graphen erstellen, die mir geholfen haben, die Daten besser zu verstehen. Auch wenn man nicht jeden Graphen mit dieser Funktion visualisieren kann, war sie meiner Meinung nach hilfreich und informativ genug, sodass ich mich entschieden habe, diese Funktion in meinem Dashboard für die spielerische Nutzung zu integrieren, bzw. drinnen zu lassen.\n\nFür die RevealJS Präsentation und dem Streamlit Dashbaord habe ich noch zwei weitere Graphen erstellt und diese ähnlich wie die erste in einer Funktion zum einfachen Aufrufen verpackt. Ein Graph war die Länderkarte, welche man mit der create_chart() Funktion nicht ohne weiteres erstellen konnte. Hierzu habe ich dann, ebenfalls mit der Hilfe von ChatGPT, diese Funktion erstellt (verkürzt, Error-Handling, Chart-Mapping und x-/y-encoding genau wie bei create_chart()):\n\n\ndef create_map_chart(df=df, mark_column='', sel_locs=[], aggregation='sum', marker_type='point', title=''):\n    \n    # ...\n    # US states background\n    states = alt.topo_feature(data.us_10m.url, 'states')\n    background = alt.Chart(states).mark_geoshape(\n        fill='lightgray',\n        stroke='white',\n    ).encode(\n        tooltip=alt.value(None)\n    ).properties(\n        title=title,\n    ).project('albersUsa')\n\n    # data points\n    points = marker_type_mapping[marker_type].encode(\n        longitude='Longitude:Q',\n        latitude='Latitude:Q',\n        size=alt.Size(c_enc, title=title),\n        color='Location:N',\n        tooltip=['Location:N', alt.Tooltip(c_enc, format='.2f')],\n    ).interactive()\n\n    # selected states\n    text_labels = alt.Chart(filtered_df).mark_text(dx=0, dy=-13, color='black').encode(\n        longitude='Longitude:Q',\n        latitude='Latitude:Q',\n        text=alt.Text(f'{c_enc}', format='.2f'),\n        tooltip=alt.value(None)\n    )\n\n    final_map_chart = background + points + text_labels\n    return final_map_chart\n\n\n\nDiese Funktion funktioniert ähnlich wie die create_chart() Funktion nur, dass diese Funktion ein Altair Chart returned, welches aus drei Schichten besteht. Die Karte ist der lediglich der Hintergrund (Staatsgrenzen), während die Punkte innerhalb der einzelnen Staaten der eigentliche informative Graph sind. Diese Funktion funktioniert ausschließlich für Chart Maps.\n\nDie andere Funktion war die „create_subscription_grouped_chart“, welche einen gruppierten Bar Chart erstellt. Dieser Chart wurde auch nur einmal aufgezeigt, weshalb es hier für einen geringeren Aufwand keine Parameter für Variationen gab. Der Grouped Chart hatte die Aufgabe, aufzuzeigen, wie sich die genutzten Discounts auf die Kunden mit und ohne Subscription (oder Premium Status) verteilen:\n\n\ndef create_subscription_grouped_chart():\n    grouped_df = df.groupby(['Subscription Status', 'Discount Applied'], observed=True)[\n        'Purchase Amount (USD)'].sum().reset_index()\n\n    sp_chart = alt.Chart(grouped_df).mark_bar().encode(\n        x='Discount Applied:N',\n        y='sum(Purchase Amount (USD)):Q',\n        color='Subscription Status:N',\n        tooltip=['Subscription Status:N', 'Discount Applied:N',\n                 'sum(Purchase Amount (USD)):Q']\n    ).properties(\n        width='container',\n        title='Sum of Purchase Amount (USD) per Subscription Status by Discount Applied'\n    )\n\n    return sp_chart"
  },
  {
    "objectID": "building-process.html#revealjs-aufbau",
    "href": "building-process.html#revealjs-aufbau",
    "title": "2  Erstellungsprozess",
    "section": "2.3 RevealJS Aufbau",
    "text": "2.3 RevealJS Aufbau\nMithilfe dieser drei Funktionen war ich in der Lage die RevealJS Präsentation und das Streamlit Dashboard aufzubauen. Dadurch, dass die Graphen anhand der in die jeweiligen Funktion übergebene Parameter aufgebaut und individualisiert werden, konnten sie für statische als auch dynamische Darstellungen der Graphen benutzt werden. Aufgrund dessen konnte ich sowohl bei der Quarto Präsentation als auch beim Streamlit Dashboard diese Funktionen gleichermaßen nutzen und musste nur die Parameter anpassen. Durch die Quarto Markdown Syntax und CSS Style-Rules konnte ich das Layout einer Slide in der Basis so aufbauen:\n\n\ncreate_chart(df=df, x='Subscription Status', y='Previous Purchases',\n             aggregation='average', chart_type='bar', show_text=True)\n\n\nBei einigen Slides waren noch einige Text und andere Visualisierungselemente enthalten die ich einfach mit dem aus der create_chart() Funktion erstellten Chart kombinieren konnte:\n\n\nchart_ip = create_chart(df=df, x='Item Purchased', y='Purchase Amount (USD)',\n                        aggregation='sum', chart_type='scatter', sort_order='ascending', lower_threshold=8000, upper_threshold=10000)\n\n# Hinzufügen der waagrechten Linie für das untere Threshold\nlower_line = alt.Chart(pd.DataFrame({'y': [8000]})).mark_rule(\n    color='red').encode(y='y:Q')\n\n# Hinzufügen der waagrechten Linie für das obere Threshold\nupper_line = alt.Chart(pd.DataFrame({'y': [10000]})).mark_rule(\n    color='green').encode(y='y:Q')\n\n# Kombinieren von Chart, Linie und Text mit der layer-Methode\nfinal_chart = alt.layer(chart_ip, lower_line,\n                        upper_line).properties(width='container')\n\nfinal_chart\n\n\nHier wurde beispielsweise noch eine Threshold Linie für zwei Werte erstellt die, dank des Altair Frameworks, ganz einfach mit alt.layer() kombiniert werden konnten, wobei hier die Chart aus der create_chart() Funktion immer die Basis darstellt.\nKombiniert mit der Quarto Markdown Syntax konnte ich somit recht einfach eine Slide in meiner Präsentation aufbauen. Dies war mir sehr wichtig, da bei der Homework dazu am Ende eine 800 Zeilen Datei rauskam, die zunächst für mich persönlich sehr schwer zu überschauen war, ich aber auch da den funktionalen Ansatz genommen habe und somit die „Fehler“ dieses Mal ausbessern und mich wesentlich einfacher zurechtfinden konnte.\nSlides, bei der keine Graphen vorkommen wurden mit Quarto Markdown und HTML zusammengebaut."
  },
  {
    "objectID": "building-process.html#streamlit-aufbau",
    "href": "building-process.html#streamlit-aufbau",
    "title": "2  Erstellungsprozess",
    "section": "2.4 Streamlit Aufbau",
    "text": "2.4 Streamlit Aufbau\nGleichermaßen konnte ich das Dashboard mit der Funktion recht einfach aufbauen:\n\n\ndf = f.get_df()\n\nst.sidebar.title('Filter')\n\n# inputs\nx_axis = st.sidebar.selectbox(\"Select X-axis:\", df.columns)\ny_axis_list = list(set(df.columns).difference([x_axis]))\n\ndefault_y_axis = 'Purchase Amount (USD)'\n\nif x_axis != default_y_axis:\n    y_axis_list.remove(default_y_axis)\n    y_axis_list.insert(0, default_y_axis)\n\ny_axis = st.sidebar.selectbox(\n    \"Select Y-axis:\", y_axis_list)\n\nchart_type = st.sidebar.radio(\n    \"Select Chart Type:\", ['bar', 'point', 'line', 'area', 'tick', 'scatter', 'boxplot'])\n\nsorting = None\naggregation = ''\nif df[y_axis].dtype in ['int64', 'float64']:\n    sorting = st.sidebar.radio(\"Select sorting order:\", [\n        None, 'ascending', 'descending'])\n\n    aggregation = st.sidebar.selectbox(\"Select Aggregation:\", [\n        'sum', 'average', 'count', 'min', 'max', 'median', 'stdev', 'variance'])\n\n# --------------------------------------------------------- PAGE\n\n# ...\n\ntry:\n    st.altair_chart(f.create_chart(df=df, x=x_axis, y=y_axis, aggregation=aggregation,\n                    chart_type=chart_type, sort_order=sorting), use_container_width=True)\nexcept Exception as e:\n    st.write(f'{e}')\n\n\nIn diesem Beispiel musste ich lediglich durch Streamlit Widgets und vereinzelt weiterführender Logik die Parameter sammeln und in die create_chart() übergeben. Das dataframe und die Funktion werden von einer externen py-Datei geladen, in der alle Funktionen gesammelt sind. Dabei sind alle Funktionen gecached, damit die App insgesamt perfomanter läuft:\n\n\n@st.cache_data\ndef get_df():\n    # ...\n\n@st.cache_data\ndef create_chart(**kwargs):\n    # ...\n\n\nDa die create_chart() Funktion ursprünglich für diesen Anwendungsfall gebaut war, lief das ohne großartige Probleme und ein Chart konnte somit sehr einfach zusammengestellt werden.\nDie Struktur meines Dashboard war so aufgebaut:\n\n\n\napp.py\n\nimport streamlit as st\nfrom playground import playground\nfrom country_overview import country_overview\nfrom shopping_trends import shopping_trends\n\ndef main():\n    # Create a navigation bar to switch between pages\n    selected_page = st.sidebar.radio(\n        \"Select Page\", [\"Shopping Trends\", \"Country Overview\", \"Playground\"])\n\n    # Render the selected page\n    if selected_page == \"Country Overview\":\n        country_overview()\n    elif selected_page == \"Shopping Trends\":\n        shopping_trends()\n    elif selected_page == \"Playground\":\n        playground()\n\nif __name__ == \"__main__\":\n    main()\n\n\nEine Ansicht war durch den Radio Input immer eine existierende Seite. Standardgemäß ist die Zusammenfassung der Präsentation ausgewählt und über die anderen Radio Buttons kann man ganz einfach auf die anderen Seiten des Dashboard navigieren. Dabei war jede Seite in einer Funktion verpackt, die hier je nach ausgewählten Radio ausgeführt und somit für den Nutzer gerendet wird."
  },
  {
    "objectID": "learnings.html#fazit",
    "href": "learnings.html#fazit",
    "title": "3  Fazit & Gelerntes",
    "section": "3.1 Fazit",
    "text": "3.1 Fazit"
  },
  {
    "objectID": "learnings.html#gelerntes",
    "href": "learnings.html#gelerntes",
    "title": "3  Schlusswörter",
    "section": "3.2 Gelerntes",
    "text": "3.2 Gelerntes\nEines der Dinge, die ich besser hätte machen können, war die Reihenfolge der präsentierten Informationen. In diesem Datensatz hatte ich von Länderdaten bis hin zu dem eigentlichen Produkt mehrere Ebenen, die ich mit dem Fortschritt der Präsentation immer abstufen könnte, also beispielsweise: Location -&gt; Category -&gt; Color -&gt; Item Purchased -&gt; … oder so ähnlich.\nAußerdem habe nun ein ungefähres Gefühl, mit welchen Graphentypen man am besten welche Art von Daten am besten darstellt. Hierbei hätte ich im Nachhinein gerne mehr Zeit reingesteckt, um mir mit dieser Aussage sicher zu sein. Jedoch denke ich, dass ich lediglich nur genug Übung brauche, um ein Gefühl für das Datastory Telling zu erhalten.\nAuch wenn, wie zuvor erwähnt, mir nicht jedes Framework, dass in diesem Modul benutzt werden sollte, gefallen bzw. gelegen hat, habe ich dennoch Erfahrungen mit den Tools gesammelt und Streamlit sogar in anderen Modulen angewandt, wodurch ich meine Fertigkeiten im besagten Tool verbessern konnte. Kompliziert aber dennoch sehr spannend war dabei zu lernen, wie das rendering Lifecycle in Streamlit funktioniert, wodurch ich nun in der Lage bin, kompliziertere Apps zu bauen und durch das Design des Frameworks geht das sogar auch recht schnell."
  },
  {
    "objectID": "learnings.html#fazit-herausforderungen",
    "href": "learnings.html#fazit-herausforderungen",
    "title": "3  Schlusswörter",
    "section": "3.1 Fazit & Herausforderungen",
    "text": "3.1 Fazit & Herausforderungen\nEine besondere Herausforderung war eine richtige Story aus den Erkenntnissen der Analyse und den erstellten Graphen zu bilden. Wie schon anfangs erwähnt gibt es keine nützlichen zeitlichen Informationen, weshalb es unmöglich für mich war die Entwicklung der Daten oder zeitlich abhängige Trends Stück für Stück dem Publikum zu veranschaulichen.\nDurch den funktionellen Ansatz, den ich von anfang an verfolgt hatte, war es gar nicht so kompliziert, die Inhalte der Präsentation in das Dashbaord zu platzieren und andersrum. Dies war für das Projekt äußerst hilfreich, da es so schon viele Inhalte sind, die man in der Präsentation und dem Dashboard platzieren muss und mann dadurch den Inhalt nicht ständig auf das andere Tool anpassen musste. Dadurch war der Aufbau nicht durch sonderlich große Probleme gestört worden.\nWas mir sehr gefallen und zugleich nicht gefallen hat, waren die Frameworks mit denen wir unser Projekt aufbauen sollten. Quarto war dabei für mich leider ziemlich fehleranfällig, weshalb ich beim Aufbau der RevealJS Präsentation sehr viel Troubleshooting betreiben musste. Die ständigen Fehler waren teils sehr frustrierend, weshalb ich zukünftig ungerne damit arbeiten würde. Streamlit hingegen war ein für mich sehr überraschendes neues Tool, da man hier sehr einfach eine interaktive Nutzeroberfläche erstellen kann und dies mit Python umsetzt, wodurch einige Anwendungsfälle einfach umgesetzt werden können (Beispielsweise ein KI-Chatbot, welches nicht durch eine OpenAI API oder ähnlichem funktioniert)."
  }
]